{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10023826,"sourceType":"datasetVersion","datasetId":6172714}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install transformers, accelerate, and bitsandbytes for 8-bit quantization\n!pip install transformers accelerate bitsandbytes peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nimport torch\n\n# Load the CSV file\ndf = pd.read_csv('/kaggle/input/new-qna/qna.csv')\ndf = df.iloc[:, :2]\n\n# Display the first few rows\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\nmodel_name = 'openai-community/gpt2'\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token\n\n# Configure 8-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\n# Load the model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16\n)\n\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Combine question and answer into a single string\ndef format_example(row):\n    return f\"Question: {row['Question']}\\nAnswer: {row['Answer']}\"\n\ndf['text'] = df.apply(format_example, axis=1)\n\n# Create a Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['text']])\n\n# Split the dataset if needed (e.g., train/test)\n# dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize the dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n\n# Set the format for PyTorch\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank of the LoRA matrices; adjust as needed\n    lora_alpha=32,\n    target_modules=[\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # Modules to apply LoRA; adjust based on model architecture\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Wrap the model with PEFT\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=100,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,  # Higher learning rate for LoRA layers\n    fp16=True,\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=2,\n    evaluation_strategy='no',\n    optim=\"paged_adamw_8bit\",  # Use 8-bit optimizer\n    ddp_find_unused_parameters=False  # Important for PEFT\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the PEFT model\nmodel.save_pretrained('./finetuned_gpt2_100_new_peft')\ntokenizer.save_pretrained('./finetuned_gpt2_100_new_peft')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.float16\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained('./finetuned_gpt2_100_new_peft')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(base_model, './finetuned_gpt2_100_new_peft')\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_answer(question, max_length=200):\n    input_text = f\"Question: {question}\\nAnswer:\"\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n    output = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    # Extract the answer part\n    answer = answer.split(\"Answer:\")[1].strip()\n    return answer\n\n# Test the model\nquestion = \"How can I implement obstacle avoidance in ROS2 using Nav2?\"\nanswer = generate_answer(question)\nprint(\"Question:\", question)\nprint(\"Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom huggingface_hub import HfApi\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Set your Hugging Face API token\nos.environ['HUGGINGFACE_HUB_TOKEN'] = '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'\n\n# Initialize the HfApi with your token\napi = HfApi(token=os.environ['HUGGINGFACE_HUB_TOKEN'])\n\n# Verify authentication\nuser_info = api.whoami()\nusername = user_info['name']\nprint(f\"Logged in as: {username}\")\n\n# Define repository details\nrepo_name = \"finetuned-gpt2_100_new-peft\"\nrepo_id = f\"{username}/{repo_name}\"\n\n# Create the repository\napi.create_repo(repo_id=repo_id, private=False, exist_ok=True)\n\n# Push the model and tokenizer to the repository\nmodel.push_to_hub(repo_id, token=os.environ['HUGGINGFACE_HUB_TOKEN'])\ntokenizer.push_to_hub(repo_id, token=os.environ['HUGGINGFACE_HUB_TOKEN'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}